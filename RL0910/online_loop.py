"""
online_loop.py - Online incremental training loop with active learning
"""

import torch
import torch.nn as nn
import numpy as np
from typing import Dict, List, Optional, Callable, Tuple
from collections import deque
import time
import os
from datetime import datetime
import threading
import queue
import json
from threading import Lock
from typing import Callable
from datetime import datetime
import random
from models import TransformerDynamicsModel, TreatmentOutcomeModel, ConservativeQNetwork, EnsembleQNetwork
from training import ConservativeQLearning, DigitalTwinTrainer, OutcomeModelTrainer
from samplers import StreamActiveLearner
try:
    from d3rlpy import load_learnable
    from d3rlpy.dataset import MDPDataset
    import d3rlpy
    BCQ_AVAILABLE = True
except ImportError:
    BCQ_AVAILABLE = False
    print("Warning: d3rlpy not available, falling back to CQL")


# class BCQOnlineTrainer:
#     """BCQçš„çœŸæ­£åœ¨çº¿è®­ç»ƒå®ç°"""
#     def __init__(self, bcq_policy_path: str, device: str = 'cuda'):
#         if not BCQ_AVAILABLE:
#             raise ImportError("d3rlpy not available for BCQ")

#         # ä¸ºäº†é¿å…åœ¨ä¸¤ä¸ªåŠ è½½è·¯å¾„ä¸­é‡å¤åˆå§‹åŒ–ä»£ç ï¼Œå®šä¹‰ä¸€ä¸ªå†…éƒ¨è¾…åŠ©å‡½æ•°
#         def _post_load_initialization(self):
#             # è®°å½•ç¦»çº¿ BCQ æ¨¡å‹çš„åŠ¨ä½œç©ºé—´å¤§å°
#             self._bcq_built = False
#             self.bcq_action_size = None
#             try:
#                 cfg = getattr(self.bcq_algo, "config", None)
#                 self.bcq_action_size = getattr(cfg, "action_size", None)
#                 if self.bcq_action_size is None and getattr(self.bcq_algo, "impl", None) is not None:
#                     self.bcq_action_size = getattr(self.bcq_algo.impl, "action_size", None)
#             except Exception:
#                 pass
#             if self.bcq_action_size is not None:
#                 print(f"[BCQ] Loaded policy expects action_size={self.bcq_action_size}")

#             # ä¿è¯è®­ç»ƒçº¿ç¨‹å¯ç”¨çš„ç¼“å†²åŒºå±æ€§
#             if not hasattr(self, "labeled_buffer"):
#                 self.labeled_buffer = []
#             self.online_buffer = self.labeled_buffer # åšæˆå¼•ç”¨
            
#             # äº’æ–¥é”å’Œè®¡æ•°å™¨
#             if not hasattr(self, "_buffer_lock"):
#                 self._buffer_lock = threading.Lock()
#             self._transition_count = 0
#             self._manual_transition_count = False

#             # æŒ‡å¯¼â‘ ï¼šè¡¥é½é»˜è®¤å­—æ®µ
#             # --- defaults for online loop bookkeeping ---
#             try:
#                 from drive_tools import CURRENT_HYPERPARAMS as _HP
#                 _default_bs = int(_HP.get("batch_size", 32))
#             except Exception:
#                 _default_bs = 32
#             self._batch_size = int(os.getenv("BCQ_BATCH_SIZE", str(_default_bs)))
            
#             # æ¯å¤šå°‘æ¡æ–°æ ·æœ¬è§¦å‘ä¸€æ¬¡æ›´æ–°ï¼›å¯ç”¨ç¯å¢ƒå˜é‡è¦†ç›–
#             self._update_frequency = max(1, int(os.getenv("BCQ_UPDATE_FREQUENCY", "20")))

#             # ç»Ÿè®¡è¾…åŠ©
#             self._update_count = 0
#             self._last_update_count = 0
#             self.updates_done = 0
#             self.update_calls = 0
#             self._last_update_walltime = time.time()
#             self.training_losses = deque(maxlen=100)
            
#         # åŠ è½½é¢„è®­ç»ƒBCQæ¨¡å‹
#         try:
#             self.bcq_algo = load_learnable(bcq_policy_path)
#             print(f"âœ“ BCQ policy loaded successfully from {bcq_policy_path}")
#             _post_load_initialization(self)
#         except Exception as e:
#             print(f"BCQ loading failed with error: {e}")
#             print("Attempting alternative loading method...")
#             try:
#                 import torch
#                 checkpoint = torch.load(bcq_policy_path, map_location=device)
#                 if hasattr(checkpoint, 'predict'):
#                     self.bcq_algo = checkpoint
#                     print("âœ“ BCQ policy loaded via torch.load")
#                     _post_load_initialization(self)
#                 else:
#                     raise Exception("Checkpoint doesn't contain BCQ algorithm")
#             except Exception as e2:
#                 print(f"Alternative loading also failed: {e2}")
#                 raise ImportError(f"Cannot load BCQ policy: {e}")

#         self.device = device
#         print(f"âœ“ BCQ online trainer ready: update_freq={self.update_frequency}, batch_size={self.batch_size}")

#     # æŒ‡å¯¼â‘¡ï¼šå¢åŠ å±æ€§ï¼ˆgetter/setterï¼‰
#     @property
#     def batch_size(self) -> int:
#         return getattr(self, "_batch_size", 32)

#     @batch_size.setter
#     def batch_size(self, value):
#         try:
#             self._batch_size = max(1, int(value))
#         except Exception:
#             self._batch_size = 32

#     @property
#     def update_frequency(self) -> int:
#         return getattr(self, "_update_frequency", 20)

#     @update_frequency.setter
#     def update_frequency(self, value):
#         try:
#             self._update_frequency = max(1, int(value))
#         except Exception:
#             self._update_frequency = 20

#     @property
#     def update_count(self) -> int:
#         return getattr(self, "_update_count", 0)

#     @update_count.setter
#     def update_count(self, value):
#         try:
#             self._update_count = int(value)
#         except Exception:
#             pass # å¿½ç•¥éæ³•èµ‹å€¼

#     @property
#     def transition_count(self):
#         if getattr(self, "_manual_transition_count", False):
#             return self._transition_count
#         try:
#             return len(self.online_buffer)
#         except Exception:
#             return self._transition_count

#     @transition_count.setter
#     def transition_count(self, value):
#         try:
#             v = int(value)
#         except Exception:
#             v = 0
#         try:
#             with self._buffer_lock:
#                 self._transition_count = v
#                 self._manual_transition_count = True
#         except Exception:
#             self._transition_count = v
#             self._manual_transition_count = True

#     def add_transition(self, transition: Dict):
#         """æ·»åŠ transitionåˆ°åœ¨çº¿buffer"""
#         formatted_transition = {
#             'observation': np.asarray(transition['state'], dtype=np.float32),
#             'action': int(transition['action']),
#             'reward': float(transition['reward']),
#             'next_observation': np.asarray(transition['next_state'], dtype=np.float32),
#             'terminal': bool(transition.get('done', False))
#         }
#         self.online_buffer.append(formatted_transition)
#         self._transition_count += 1
        
#         if (self._transition_count % self.update_frequency == 0 and 
#             len(self.online_buffer) >= self.batch_size):
#             return self.perform_update()
        
#         return {'updated': False, 'buffer_size': len(self.online_buffer)}

#     def on_update(self, steps: int = 0):
#         """æ¯æ¬¡å®Œæˆä¸€æ¬¡ bcq.fit åè°ƒç”¨ï¼Œç”¨äºè®°å½•æ›´æ–°æ¬¡æ•°ç­‰ã€‚"""
#         self.update_calls += 1
#         self._last_update_walltime = time.time()

# def perform_update(self) -> Dict:
#     try:
#         with self._buffer_lock:
#             buffer_len = len(self.online_buffer)
#             if buffer_len < self.batch_size:
#                 return {'updated': False, 'reason': 'not enough samples'}

#             idx = np.random.choice(buffer_len, self.batch_size, replace=False)
#             batch_data = [self.online_buffer[i] for i in idx]

#         # --------- ç»„ batch å¼ é‡ ---------
#         observations = np.asarray([t['observation'] for t in batch_data], dtype=np.float32)
#         next_observations = np.asarray([t['next_observation'] for t in batch_data], dtype=np.float32)
#         actions_raw = np.asarray([int(t['action']) for t in batch_data], dtype=np.int64)
#         rewards = np.asarray([float(t['reward']) for t in batch_data], dtype=np.float32)
#         terminals = np.asarray([bool(t.get('terminal', t.get('done', False))) for t in batch_data], dtype=np.bool_)
#         timeouts = np.zeros_like(terminals, dtype=np.bool_)
#         if not terminals.any():
#             timeouts[-1] = True

#         # --------- åŠ¨ä½œç»´åº¦æ ¡å‡† ---------
#         if getattr(self, "bcq_action_size", None):
#             active_action_dim = int(self.bcq_action_size)
#         else:
#             active_action_dim = int(actions_raw.max() + 1) if actions_raw.size > 0 else 1

#         invalid = actions_raw >= active_action_dim
#         if invalid.any():
#             keep = ~invalid
#             if keep.sum() == 0:
#                 return {'updated': False, 'reason': 'all actions out of range'}
#             observations = observations[keep]
#             next_observations = next_observations[keep]
#             rewards = rewards[keep]
#             terminals = terminals[keep]
#             timeouts = timeouts[keep]
#             actions_raw = actions_raw[keep]

#         if actions_raw.size > 0:
#             actions_raw = np.clip(actions_raw, 0, active_action_dim - 1)
#         actions = actions_raw.reshape(-1, 1)

#         # --------- æ„å»º d3rlpy MDPDatasetï¼ˆå‘åå…¼å®¹ï¼‰---------
#         try:
#             dataset = MDPDataset(
#                 observations=observations,
#                 actions=actions,
#                 rewards=rewards,
#                 terminals=terminals,
#                 timeouts=timeouts,
#                 next_observations=next_observations,
#                 discrete_action=True,
#                 action_size=active_action_dim,
#             )
#         except TypeError:
#             try:
#                 dataset = MDPDataset(
#                     observations=observations,
#                     actions=actions,
#                     rewards=rewards,
#                     terminals=terminals,
#                     timeouts=timeouts,
#                     next_observations=next_observations,
#                 )
#             except TypeError:
#                 dataset = MDPDataset(
#                     observations=observations,
#                     actions=actions,
#                     rewards=rewards,
#                     terminals=(terminals | timeouts),
#                 )

#         # --------- é¦–æ¬¡/ä¸ä¸€è‡´æ—¶å¼ºåˆ¶ build ---------
#         need_build = not getattr(self, "_bcq_built", False)
#         try:
#             impl = getattr(self.bcq_algo, "impl", None) or getattr(self.bcq_algo, "_impl", None)
#             impl_action_size = getattr(impl, "action_size", None) if impl is not None else None
#             if impl_action_size is not None and int(impl_action_size) != active_action_dim:
#                 need_build = True
#         except Exception:
#             pass

#         if need_build:
#             try:
#                 if hasattr(self.bcq_algo, "_impl"):
#                     self.bcq_algo._impl = None
#                 if hasattr(self.bcq_algo, "impl"):
#                     self.bcq_algo.impl = None
#                 self.bcq_algo.build_with_dataset(dataset)
#                 self._bcq_built = True
#                 self.bcq_action_size = active_action_dim
#                 print(f"[BCQ] Rebuilt with action_size={active_action_dim}")
#             except Exception as e_build:
#                 print(f"[BCQ] Rebuild failed (fallback to current impl): {e_build}")
#                 self._bcq_built = True  # é¿å…åå¤é‡å»º

#         # --------- è®­ç»ƒï¼ˆå¤šç‰ˆæœ¬å…¼å®¹ï¼‰---------
#         update_steps = 20
#         training_results = None
#         try:
#             training_results = self.bcq_algo.fit(
#                 dataset,
#                 n_steps=update_steps,
#                 n_steps_per_epoch=update_steps,
#                 show_progress=False,
#                 save_interval=None,
#             )
#         except TypeError:
#             try:
#                 training_results = self.bcq_algo.fit(
#                     dataset,
#                     n_steps=update_steps,
#                     n_steps_per_epoch=update_steps,
#                     show_progress=False,
#                 )
#             except TypeError:
#                 try:
#                     training_results = self.bcq_algo.fit(dataset, n_epochs=1, show_progress=False)
#                 except TypeError:
#                     training_results = self.bcq_algo.fit(dataset, show_progress=False)

#         # --------- è®°è´¦ ---------
#         self._update_count = getattr(self, "_update_count", 0) + 1
#         self.updates_done = self._update_count
#         self._last_update_count = self._update_count
#         self.on_update(update_steps)

#         # è®°å½•æŸå¤±
#         latest = None
#         if training_results is not None:
#             hist = getattr(training_results, 'history', None)
#             if isinstance(hist, dict):
#                 for k in ('loss', 'td_loss', 'imitator_loss', 'objective_loss'):
#                     v = hist.get(k)
#                     if v:
#                         latest = float(v[-1])
#                         break
#         if latest is not None:
#             self.training_losses.append(latest)

#         return {
#             'updated': True,
#             'update_count': self.update_count,
#             'buffer_size': buffer_len,
#             'avg_loss': float(np.mean(list(self.training_losses))) if self.training_losses else 0.0,
#         }

#     except Exception as e:
#         print(f"BCQ online update failed: {e}")
#         return {'updated': False, 'error': str(e)}
    
#     # This 'def' must be at the same indentation level as 'def perform_update'
#     # There should not be an open 'try' statement immediately before this line
#     def predict(self, state: np.ndarray) -> int:
#         """BCQç­–ç•¥é¢„æµ‹"""
#         try:
#             if state.ndim == 1:
#                 state = state.reshape(1, -1)
            
#             action = self.bcq_algo.predict(state.astype(np.float32))
#             return int(action[0] if hasattr(action, '__len__') else action)
#         except Exception as e:
#             print(f"BCQ prediction error: {e}")
#             return 0
    
#     def get_statistics(self) -> Dict:
#         """è·å–è®­ç»ƒç»Ÿè®¡ä¿¡æ¯"""
#         return {
#             'bcq_updates': self.update_count,
#             'bcq_buffer_size': len(self.online_buffer),
#             'bcq_avg_loss': np.mean(list(self.training_losses)) if self.training_losses else 0.0,
#             'transitions_processed': self.transition_count
#         }

# === Baseline ç®—æ³•å·¥å‚ï¼ˆDQN / DoubleDQN / NFQ / CQL / CalQL[å¯é€‰]ï¼‰===
try:
    from d3rlpy.algos import DQNConfig, DoubleDQNConfig, NFQConfig
    try:
        from d3rlpy.algos import DiscreteCQLConfig as _CQLConfig
    except Exception:
        from d3rlpy.algos import CQLConfig as _CQLConfig
except Exception as _e:
    DQNConfig = DoubleDQNConfig = NFQConfig = _CQLConfig = None
    print("[WARN] d3rlpy.algos not fully available:", _e)

# å¯é€‰ Cal-QLï¼ˆæ²¡æœ‰å°±ç”¨ CQL å…œåº•ï¼‰
_HAS_CALQL = False
try:
    from calql import CalQL as _CalQLAlgo  # æ²¡æœ‰æ­¤åŒ…ä¼š ImportError
    _HAS_CALQL = True
except Exception:
    _HAS_CALQL = False

def _build_algo_by_name(name: str, device: str = "cuda"):
    n = (name or "dqn").lower()
    if n == "dqn":
        return DQNConfig().create(device=device)
    if n == "double_dqn":
        return DoubleDQNConfig().create(device=device)
    if n == "nfq":
        return NFQConfig().create(device=device)
    if n == "calql":
        if _HAS_CALQL:
            return _CalQLAlgo(device=device)
        print("[GenericTrainer] calql package not found â€” fallback to CQL in d3rlpy.")
        return _CQLConfig().create(device=device)
    if n == "cql":
        return _CQLConfig().create(device=device)
    # é»˜è®¤å®‰å…¨é€‰ DQN
    return DQNConfig().create(device=device)


class GenericD3RLPyOnlineTrainer:
    """
    ä¸ BCQOnlineTrainer åŒæ¥å£ï¼š
      - add_transition(transition) -> dict
      - perform_update() -> dict
      - predict(state: np.ndarray) -> int
      - get_statistics() -> dict

    é€šè¿‡ç¯å¢ƒå˜é‡ RL_BASELINE é€‰æ‹© {bcq(ä¸åœ¨æ­¤ç±»)ã€dqn,double_dqn,nfq,cql,calql}
    """
    def __init__(self, algo_name: str = "dqn", device: str = "cuda"):
        import threading, time
        from collections import deque
        self.device = device
        self._algo_name = algo_name.strip().lower()
        self._batch_size = int(os.getenv("BATCH_SIZE", os.getenv("BCQ_BATCH_SIZE", 32)))
        self._update_frequency = max(1, int(os.getenv("UPDATE_FREQUENCY", os.getenv("BCQ_UPDATE_FREQUENCY", "20"))))
        self._update_steps = int(os.getenv("UPDATE_STEPS", os.getenv("BCQ_UPDATE_STEPS", "20")))

        self.labeled_buffer = []
        self.online_buffer = self.labeled_buffer
        self._buffer_lock = threading.Lock()
        self._built = False
        self._update_count = 0
        self.training_losses = deque(maxlen=200)
        self._last_update_walltime = time.time()

        # åˆå§‹åŒ–ç®—æ³•å®ä¾‹ï¼ˆæœª buildï¼›ç”¨é¦–æ‰¹è¦†ç›–æ‰€æœ‰åŠ¨ä½œçš„æ ·æœ¬æ¥ buildï¼‰
        self.algo = _build_algo_by_name(self._algo_name, device=self.device)
        print(f"[GenericTrainer] algo={self._algo_name} device={self.device} "
              f"(batch={self._batch_size}, freq={self._update_frequency}, steps={self._update_steps})")

    # ä¸åœ¨çº¿å¾ªç¯å…¼å®¹çš„å±æ€§
    @property
    def batch_size(self): return int(self._batch_size)
    @batch_size.setter
    def batch_size(self, v):
        try: self._batch_size = max(1, int(v))
        except: pass

    @property
    def update_frequency(self): return int(self._update_frequency)
    @update_frequency.setter
    def update_frequency(self, v):
        try: self._update_frequency = max(1, int(v))
        except: pass

    @property
    def update_count(self): return int(self._update_count)

    # ---------- å†…éƒ¨å·¥å…· ----------
    def _make_dataset(self, samples):
        import numpy as np
        from d3rlpy.dataset import MDPDataset
        obs = np.asarray([s["observation"] for s in samples], dtype=np.float32)
        next_obs = np.asarray([s["next_observation"] for s in samples], dtype=np.float32)
        acts = np.asarray([int(s["action"]) for s in samples], dtype=np.int64).reshape(-1, 1)
        rews = np.asarray([float(s["reward"]) for s in samples], dtype=np.float32)
        terms = np.asarray([bool(s["terminal"]) for s in samples], dtype=np.bool_)
        timeouts = np.zeros_like(terms, dtype=np.bool_)
        if not terms.any():
            timeouts[-1] = True
        try:
            return MDPDataset(observations=obs, actions=acts, rewards=rews,
                              terminals=terms, timeouts=timeouts, next_observations=next_obs)
        except TypeError:
            try:
                return MDPDataset(observations=obs, actions=acts, rewards=rews,
                                  terminals=terms, timeouts=timeouts)
            except TypeError:
                return MDPDataset(observations=obs, actions=acts, rewards=rews,
                                  terminals=(terms | timeouts))

    def _first_build_indices(self):
        # é€‰ä¸€æ‰¹æ ·æœ¬è¦†ç›–â€œå·²å‡ºç°çš„æ‰€æœ‰åŠ¨ä½œç±»åˆ«â€ï¼Œé¿å… action_size è¿‡å°
        import random
        first_idx = {}
        for i, s in enumerate(self.online_buffer):
            a = int(s["action"])
            if a not in first_idx:
                first_idx[a] = i
        idxs = list(first_idx.values())
        if len(idxs) < min(self._batch_size, len(self.online_buffer)):
            rest = [i for i in range(len(self.online_buffer)) if i not in idxs]
            random.shuffle(rest)
            need = min(self._batch_size, len(self.online_buffer)) - len(idxs)
            idxs += rest[:need]
        return idxs

    def _ensure_built(self):
        if self._built or len(self.online_buffer) == 0:
            return
        idxs = self._first_build_indices()
        ds = self._make_dataset([self.online_buffer[i] for i in idxs])
        try:
            self.algo.build_with_dataset(ds)
        except Exception:
            pass
        self._built = True

    # ---------- å¯¹å¤–æ¥å£ ----------
    def add_transition(self, transition: dict):
        import numpy as np
        try:
            t = {
                "observation": np.asarray(transition["state"], dtype=np.float32),
                "action": int(transition["action"]),
                "reward": float(transition["reward"]),
                "next_observation": np.asarray(transition["next_state"], dtype=np.float32),
                "terminal": bool(transition.get("done", transition.get("terminal", False))),
            }
        except Exception as e:
            return {"updated": False, "error": f"bad transition: {e}"}

        with self._buffer_lock:
            self.online_buffer.append(t)
            buf_len = len(self.online_buffer)
            need_update = (buf_len % self._update_frequency == 0) and (buf_len >= self._batch_size)

        if need_update:
            return self.perform_update()
        return {"updated": False, "buffer_size": buf_len}

    def perform_update(self):
        import numpy as np
        with self._buffer_lock:
            buf_len = len(self.online_buffer)
            if buf_len == 0:
                return {"updated": False, "reason": "empty buffer"}
            # å…ˆä¿è¯æŒ‰â€œå…¨åŠ¨ä½œè¦†ç›–â€åšä¸€æ¬¡ build
            self._ensure_built()
            size = min(self._batch_size, buf_len)
            batch_idxs = np.random.choice(buf_len, size=size, replace=False)
            batch = [self.online_buffer[i] for i in batch_idxs]

        ds = self._make_dataset(batch)
        tr = None
        try:
            tr = self.algo.fit(ds, n_steps=self._update_steps,
                               n_steps_per_epoch=self._update_steps,
                               show_progress=False, save_interval=None)
        except TypeError:
            try:
                tr = self.algo.fit(ds, n_steps=self._update_steps,
                                   n_steps_per_epoch=self._update_steps,
                                   show_progress=False)
            except TypeError:
                try:
                    tr = self.algo.fit(ds, n_epochs=1, show_progress=False)
                except TypeError:
                    tr = self.algo.fit(ds, show_progress=False)
        except Exception as e:
            msg = str(e)
            # åŠ¨ä½œç»´åº¦ä¸å¤Ÿï¼šé‡å»º + è¦†ç›–
            if "Class values must be smaller than num_classes" in msg:
                with self._buffer_lock:
                    cov_idxs = self._first_build_indices()
                    cov = [self.online_buffer[i] for i in cov_idxs]
                ds_cov = self._make_dataset(cov)
                # é‡æ–°åˆ›å»ºç®—æ³•å¹¶ build
                self.algo = _build_algo_by_name(self._algo_name, device=self.device)
                try:
                    self.algo.build_with_dataset(ds_cov)
                except Exception:
                    pass
                # å†è¯•ä¸€æ¬¡
                tr = self.algo.fit(ds, n_steps=self._update_steps,
                                   n_steps_per_epoch=self._update_steps,
                                   show_progress=False)
            else:
                print(f"[GenericTrainer] update failed: {e}")
                return {"updated": False, "error": msg, "buffer_size": buf_len}

        self._update_count += 1
        # è®°å½•æŸå¤±
        try:
            if tr and hasattr(tr, "history"):
                hist = getattr(tr, "history", {})
                last = None
                if isinstance(hist, dict):
                    for k in ("loss", "td_loss", "imitator_loss"):
                        if k in hist and hist[k]:
                            last = float(hist[k][-1])
                            break
                if last is not None:
                    self.training_losses.append(last)
        except Exception:
            pass

        avg_loss = float(np.mean(list(self.training_losses))) if self.training_losses else 0.0
        return {"updated": True, "update_count": self._update_count,
                "buffer_size": buf_len, "avg_loss": avg_loss}

    def predict(self, state: np.ndarray) -> int:
        import numpy as np
        try:
            x = np.asarray(state, dtype=np.float32)
            if x.ndim == 1:
                x = x.reshape(1, -1)
            a = self.algo.predict(x)
            return int(a[0] if hasattr(a, "__len__") else a)
        except Exception:
            return 0

    def get_statistics(self):
        import numpy as np
        return {
            "algo": self._algo_name,
            "updates_done": int(self._update_count),
            "buffer_size": int(len(self.online_buffer)),
            "avg_loss": float(np.mean(list(self.training_losses))) if self.training_losses else 0.0,
        }



class BCQOnlineTrainer:
    """BCQ çš„çœŸæ­£åœ¨çº¿è®­ç»ƒå®ç°ï¼ˆå»é‡ + å…¼å®¹ d3rlpy å¤šç‰ˆæœ¬ + åŠ¨ä½œè¶Šç•Œé˜²æŠ¤ + ç»Ÿè®¡ï¼‰"""
    def __init__(self, bcq_policy_path: str, device: str = 'cuda'):
        if not BCQ_AVAILABLE:
            raise ImportError("d3rlpy not available for BCQ")

        # å°è£…åç½®åˆå§‹åŒ–ï¼Œä¾¿äºä¸¤ç§åŠ è½½è·¯å¾„å…¬ç”¨
        def _post_load_initialization(self):
            # è®°å½•ç¦»çº¿ BCQ æ¨¡å‹çš„åŠ¨ä½œç©ºé—´å¤§å°
            self._bcq_built = False
            self.bcq_action_size = None
            try:
                cfg = getattr(self.bcq_algo, "config", None)
                self.bcq_action_size = getattr(cfg, "action_size", None)
                if self.bcq_action_size is None and getattr(self.bcq_algo, "impl", None) is not None:
                    self.bcq_action_size = getattr(self.bcq_algo.impl, "action_size", None)
            except Exception:
                pass
            if self.bcq_action_size is not None:
                print(f"[BCQ] Loaded policy expects action_size={self.bcq_action_size}")

            # ç»éªŒç¼“å†²åŒº + äº’æ–¥é”
            if not hasattr(self, "online_buffer"):
                from collections import deque
                self.online_buffer = deque(maxlen=10000)
            if not hasattr(self, "_buffer_lock"):
                import threading
                self._buffer_lock = threading.Lock()

            # è®¡æ•° / è¶…å‚ï¼ˆå…ç¯å¢ƒå˜é‡è¦†ç›–ï¼‰
            import os, time
            self._transition_count = 0
            self._manual_transition_count = False
            try:
                from drive_tools import CURRENT_HYPERPARAMS as _HP
                _default_bs = int(_HP.get("batch_size", 32))
            except Exception:
                _default_bs = 32
            self._batch_size = int(os.getenv("BCQ_BATCH_SIZE", str(_default_bs)))
            self._update_frequency = max(1, int(os.getenv("BCQ_UPDATE_FREQUENCY", "20")))
            self._update_count = 0
            self._last_update_count = 0
            self.updates_done = 0
            self.update_calls = 0
            self._last_update_walltime = time.time()
            from collections import deque as _dq
            self.training_losses = _dq(maxlen=100)

        # å…ˆèµ° d3rlpy.load_learnableï¼Œå¤±è´¥å†å°è¯• torch.load
        try:
            self.bcq_algo = load_learnable(bcq_policy_path)
            print(f"âœ“ BCQ policy loaded successfully from {bcq_policy_path}")
            _post_load_initialization(self)
        except Exception as e:
            print(f"BCQ loading failed with error: {e}")
            print("Attempting alternative loading method...")
            try:
                import torch
                checkpoint = torch.load(bcq_policy_path, map_location=device)
                if hasattr(checkpoint, 'predict'):
                    self.bcq_algo = checkpoint
                    print("âœ“ BCQ policy loaded via torch.load")
                    _post_load_initialization(self)
                else:
                    raise Exception("Checkpoint doesn't contain BCQ algorithm")
            except Exception as e2:
                print(f"Alternative loading also failed: {e2}")
                raise ImportError(f"Cannot load BCQ policy: {e}")

        self.device = device
        print(f"âœ“ BCQ online trainer ready: update_freq={self.update_frequency}, batch_size={self.batch_size}")

    # --- å±æ€§å°è£…ï¼ˆå¤–éƒ¨è®¿é—®è¿™äº›å­—æ®µæ—¶ä¸å†æŠ¥ AttributeErrorï¼‰ ---
    @property
    def batch_size(self) -> int:
        return getattr(self, "_batch_size", 32)
    @batch_size.setter
    def batch_size(self, value):
        try:
            self._batch_size = max(1, int(value))
        except Exception:
            self._batch_size = 32

    @property
    def update_frequency(self) -> int:
        return getattr(self, "_update_frequency", 20)
    @update_frequency.setter
    def update_frequency(self, value):
        try:
            self._update_frequency = max(1, int(value))
        except Exception:
            self._update_frequency = 20

    @property
    def update_count(self) -> int:
        return getattr(self, "_update_count", 0)
    @update_count.setter
    def update_count(self, value):
        try:
            self._update_count = int(value)
        except Exception:
            pass

    @property
    def transition_count(self):
        if getattr(self, "_manual_transition_count", False):
            return self._transition_count
        try:
            return len(self.online_buffer)
        except Exception:
            return getattr(self, "_transition_count", 0)
    @transition_count.setter
    def transition_count(self, value):
        try:
            v = int(value)
        except Exception:
            v = 0
        try:
            with self._buffer_lock:
                self._transition_count = v
                self._manual_transition_count = True
        except Exception:
            self._transition_count = v
            self._manual_transition_count = True

    # --- åœ¨çº¿æ•°æ®æ¥å…¥ ---
    def add_transition(self, transition: Dict):
        """æ·»åŠ  transition åˆ°åœ¨çº¿ bufferï¼›æŒ‰é¢‘ç‡è§¦å‘è®­ç»ƒ"""
        import numpy as _np
        formatted = {
            'observation': _np.asarray(transition['state'], dtype=_np.float32),
            'action': int(transition['action']),
            'reward': float(transition['reward']),
            'next_observation': _np.asarray(transition['next_state'], dtype=_np.float32),
            'terminal': bool(transition.get('done', False))
        }
        with self._buffer_lock:
            self.online_buffer.append(formatted)
            self._transition_count += 1

        if (self._transition_count % self.update_frequency == 0 and
            len(self.online_buffer) >= self.batch_size):
            return self.perform_update()

        return {'updated': False, 'buffer_size': len(self.online_buffer)}

    def on_update(self, steps: int = 0):
        """æ¯æ¬¡ fit() ååšè®°å½•"""
        import time as _t
        self.update_calls += 1
        self._last_update_walltime = _t.time()

    # --- å…³é”®ï¼šæ‰§è¡Œä¸€æ¬¡å°æ­¥å¢é‡è®­ç»ƒ ---
    def perform_update(self) -> Dict:
        import numpy as _np
        try:
            # é‡‡æ ·æ‰¹æ¬¡
            with self._buffer_lock:
                buf_len = len(self.online_buffer)
                if buf_len < self.batch_size:
                    return {'updated': False, 'reason': 'not enough samples'}
                idx = _np.random.choice(buf_len, self.batch_size, replace=False)
                batch = [self.online_buffer[i] for i in idx]

            obs  = _np.stack([t['observation']      for t in batch]).astype(_np.float32)
            nobs = _np.stack([t['next_observation'] for t in batch]).astype(_np.float32)
            act_raw = _np.asarray([int(t['action']) for t in batch], dtype=_np.int64)
            rew  = _np.asarray([float(t['reward'])  for t in batch], dtype=_np.float32)
            terminal = _np.asarray([bool(t['terminal']) for t in batch], dtype=_np.bool_)
            timeout  = _np.zeros_like(terminal, dtype=_np.bool_)
            if not terminal.any():
                timeout[-1] = True

            # åŠ¨ä½œè¶Šç•Œé˜²æŠ¤ï¼ˆé¿å… â€œClass values must be smaller than num_classes.â€ï¼‰
            if getattr(self, "bcq_action_size", None) is not None and act_raw.size > 0:
                # è¿‡æ»¤æ‰ >= action_size çš„éæ³•æ ·æœ¬ï¼›è‹¥å…¨éæ³•åˆ™è·³è¿‡æœ¬æ¬¡æ›´æ–°
                keep = act_raw < int(self.bcq_action_size)
                if not keep.any():
                    return {'updated': False, 'reason': 'all actions out of range'}
                obs, nobs, rew, terminal, timeout, act_raw = \
                    obs[keep], nobs[keep], rew[keep], terminal[keep], timeout[keep], act_raw[keep]

            # d3rlpy å¯¹ç¦»æ•£åŠ¨ä½œæœŸæœ› (N,1) int64
            actions = act_raw.reshape(-1, 1).astype(_np.int64)

            # æ„å»º MDPDatasetï¼ˆå…¼å®¹ä¸åŒç‰ˆæœ¬å‚æ•°ï¼‰
            try:
                dataset = MDPDataset(
                    observations=obs, actions=actions, rewards=rew,
                    terminals=terminal, timeouts=timeout, next_observations=nobs
                )
            except TypeError:
                try:
                    dataset = MDPDataset(
                        observations=obs, actions=actions, rewards=rew,
                        terminals=terminal, timeouts=timeout
                    )
                except TypeError:
                    dataset = MDPDataset(
                        observations=obs, actions=actions, rewards=rew,
                        terminals=(terminal | timeout)
                    )

            # é¦–æ¬¡ç”¨æ•°æ®é›† buildï¼ˆé”å®š action_size ç­‰å†…éƒ¨å®ç°ï¼‰ï¼Œé¿å…åç»­ batch ä¸ä¸€è‡´
            if not getattr(self, "_bcq_built", False):
                try:
                    self.bcq_algo.build_with_dataset(dataset)
                except Exception:
                    pass
                self._bcq_built = True

            # å…¼å®¹å¤šç‰ˆæœ¬ fit() ç­¾å
            steps = 20
            result = None
            try:
                result = self.bcq_algo.fit(
                    dataset,
                    n_steps=steps, n_steps_per_epoch=steps,
                    show_progress=False, save_interval=None
                )
            except TypeError:
                try:
                    result = self.bcq_algo.fit(
                        dataset,
                        n_steps=steps, n_steps_per_epoch=steps,
                        show_progress=False
                    )
                except TypeError:
                    try:
                        result = self.bcq_algo.fit(dataset, n_epochs=1, show_progress=False)
                    except TypeError:
                        result = self.bcq_algo.fit(dataset, show_progress=False)

            # ç»Ÿè®¡
            self._update_count = getattr(self, "_update_count", 0) + 1
            self.updates_done = self._update_count
            self._last_update_count = self._update_count
            self.on_update(steps)

            # è®°å½•æŸå¤±ï¼ˆä¸åŒç‰ˆæœ¬é”®ä¸åŒï¼šloss / td_loss / imitator_lossï¼‰
            if result is not None and hasattr(result, 'history'):
                hist = getattr(result, 'history', {})
                last = None
                if isinstance(hist, dict):
                    for k in ('loss', 'td_loss', 'imitator_loss', 'td_error'):
                        if k in hist and hist[k]:
                            try:
                                last = float(hist[k][-1])
                                break
                            except Exception:
                                pass
                if last is not None:
                    self.training_losses.append(last)

            return {
                'updated': True,
                'update_count': self.update_count,
                'buffer_size': buf_len,
                'avg_loss': float(_np.mean(list(self.training_losses))) if self.training_losses else 0.0
            }

        except Exception as e:
            print(f"BCQ online update failed: {e}")
            return {'updated': False, 'error': str(e)}

    def predict(self, state: np.ndarray) -> int:
        """å•æ­¥åŠ¨ä½œé¢„æµ‹"""
        try:
            if state.ndim == 1:
                state = state.reshape(1, -1)
            action = self.bcq_algo.predict(state.astype(np.float32))
            return int(action[0] if hasattr(action, '__len__') else action)
        except Exception as e:
            print(f"BCQ prediction error: {e}")
            return 0

    def get_statistics(self) -> Dict:
        """ä¾›å¤–éƒ¨ç›‘æ§/æ‰“å°"""
        import numpy as _np
        try:
            buf = len(self.online_buffer)
        except Exception:
            buf = 0
        return {
            'bcq_updates': self.update_count,
            'bcq_buffer_size': buf,
            'bcq_avg_loss': float(_np.mean(list(self.training_losses))) if self.training_losses else 0.0,
            'transitions_processed': self.transition_count
        }



class OnlineTrainer:
    """Manages online incremental training with active learning"""
    
    def __init__(self,
                 dynamics_model: TransformerDynamicsModel,
                 outcome_model: TreatmentOutcomeModel,
                 q_ensemble: EnsembleQNetwork,
                 active_learner: StreamActiveLearner,
                 batch_size: int = 32,
                 update_freq: int = 10,
                 save_freq: int = 600,  # Save every 10 minutes
                 device: str = 'cuda' if torch.cuda.is_available() else 'cpu'):
        """
        Initialize online trainer
        
        Args:
            dynamics_model: Transformer dynamics model
            outcome_model: Treatment outcome model
            q_ensemble: Ensemble Q-network
            active_learner: Active learning sampler
            batch_size: Batch size for training
            update_freq: Update models every N transitions
            save_freq: Save checkpoint every N seconds
            device: Computing device
        """
        self.dynamics_model = dynamics_model.to(device)
        self.outcome_model = outcome_model.to(device)
        self.q_ensemble = q_ensemble.to(device)
        self.active_learner = active_learner
        self.device = device
        
        # Training parameters
        self.batch_size = batch_size
        self.update_freq = update_freq
        self.save_freq = save_freq
        
        # Initialize incremental trainers
        self.dynamics_trainer = self._create_incremental_dynamics_trainer()
        self.outcome_trainer = self._create_incremental_outcome_trainer()
        
        # BCQ/CQLè®­ç»ƒå™¨åˆå§‹åŒ–
        self.use_bcq = False
        self.bcq_trainer = None
        self.q_trainers = []
        
        # æ£€æŸ¥BCQæ¨¡å‹æ˜¯å¦å­˜åœ¨
        bcq_path = os.path.join('./output/models', 'best_bcq_policy.d3')
        algo_choice = os.getenv("RL_BASELINE", "bcq").strip().lower()

        # 1) ä»æƒ³ç”¨ BCQ ä¸”æ¨¡å‹åœ¨ï¼šèµ°åŸ BCQ è·¯çº¿
        if algo_choice == "bcq" and os.path.exists(bcq_path) and BCQ_AVAILABLE:
            try:
                self.bcq_trainer = BCQOnlineTrainer(bcq_path, device)
                self.use_bcq = True
                print("âœ“ Using BCQ for online learning")
            except Exception as e:
                print(f"BCQ loading failed, fallback to baseline {algo_choice}: {e}")
                # å›è½åˆ°é€šç”¨ baselineï¼ˆé»˜è®¤ dqnï¼‰
                algo_choice = os.getenv("RL_BASELINE_FALLBACK", "dqn")
                self.bcq_trainer = GenericD3RLPyOnlineTrainer(algo_name=algo_choice, device=device)
                self.use_bcq = True
                print(f"âœ“ Using baseline algo={algo_choice} for online learning")
        # 2) æ˜ç¡®è¦æ±‚ baselineï¼šå¯ç”¨é€šç”¨ trainer
        else:
            self.bcq_trainer = GenericD3RLPyOnlineTrainer(algo_name=algo_choice, device=device)
            self.use_bcq = True
            print(f"âœ“ Using baseline algo={algo_choice} for online learning")

        self.base_tau = 0.15 if self.use_bcq else 0.1        
        # Buffers
        self.labeled_buffer = deque(maxlen=10000)
        self.weak_buffer = deque(maxlen=50000)
        self.query_buffer = deque(maxlen=1000)
        
        # Statistics
        self.stats = {
            'total_transitions': 0,
            'total_updates': 0,
            'total_queries': 0,
            'last_save_time': time.time(),
            'training_times': deque(maxlen=100)
        }
        self.evaluation_history = deque(maxlen=1000)
        # EMA for parameter stability
        self.ema_alpha = 0.99
        self._init_ema_params()
        
        # Training thread
        self.training_queue = queue.Queue()
        self.is_running = True
        self.training_thread = threading.Thread(target=self._training_loop, daemon=True)
        self.training_thread.start()
        self.training_lock = threading.Lock()
        self.last_train_time = 0
        self.shift_detector = DistributionShiftDetector()
        print("Online trainer initialized with incremental dynamics and outcome models")
        self.learning_scheduler = ProgressiveLearningScheduler()
        self.base_learning_rate = 3e-4
        # self.base_tau = 0.1
    def _create_incremental_dynamics_trainer(self) -> DigitalTwinTrainer:
        """Create trainer for incremental dynamics updates - æŒ‰ç…§è®ºæ–‡å®ç°"""
        trainer = DigitalTwinTrainer(
            self.dynamics_model,
            learning_rate=1e-4, 
            device=self.device
        )
        
        # Freeze all but last 2 layers
        self._freeze_transformer_layers(self.dynamics_model, num_trainable=2)
        
        return trainer
    def _freeze_transformer_layers(self, model: nn.Module, num_trainable: int = 2):
        """æŒ‰ç…§è®ºæ–‡å†»ç»“Transformerçš„æ—©æœŸå±‚"""
        # è·å–Transformerçš„ç¼–ç å™¨å±‚
        if hasattr(model, 'transformer'):
            encoder_layers = list(model.transformer.layers)
            
            # å†»ç»“å‰é¢çš„å±‚
            for layer in encoder_layers[:-num_trainable]:
                for param in layer.parameters():
                    param.requires_grad = False
            
            # ç¡®ä¿æœ€åå‡ å±‚å¯è®­ç»ƒ
            for layer in encoder_layers[-num_trainable:]:
                for param in layer.parameters():
                    param.requires_grad = True
        
        # å§‹ç»ˆä¿æŒè¾“å‡ºå±‚å¯è®­ç»ƒ
        if hasattr(model, 'output_projection'):
            for param in model.output_projection.parameters():
                param.requires_grad = True    

    def _create_incremental_outcome_trainer(self) -> OutcomeModelTrainer:
        """Create trainer for incremental outcome updates - æŒ‰ç…§è®ºæ–‡å®ç°"""
        trainer = OutcomeModelTrainer(
            self.outcome_model,
            learning_rate=1e-4,
            regularization_weight=0.01,
            device=self.device
        )
        
        # æŒ‰ç…§è®ºæ–‡ï¼šå†»ç»“ç¼–ç å™¨ï¼Œåªè®­ç»ƒè¾“å‡ºå¤´
        for name, param in self.outcome_model.named_parameters():
            if 'outcome_head' in name or 'treatment_discriminator' in name:
                param.requires_grad = True  # ä¿æŒè¾“å‡ºç›¸å…³å±‚å¯è®­ç»ƒ
            else:
                param.requires_grad = False  # å†»ç»“ç¼–ç å™¨
        
        return trainer
    
    def _create_incremental_q_trainers(self) -> List[ConservativeQLearning]:
            """Create trainers for each Q-network in ensemble"""
            trainers = []
            
            for q_net in self.q_ensemble.q_networks:
                trainer = ConservativeQLearning(
                    q_net,
                    [self.dynamics_model],  # å°†å•ä¸ªæ¨¡å‹åŒ…è£…ä¸ºåˆ—è¡¨
                    self.outcome_model,
                    learning_rate=3e-4,
                    cql_weight=1.0,
                    device=self.device
                )
                trainers.append(trainer)
            
            return trainers
    
    def _freeze_early_layers(self, model: nn.Module, num_trainable: int = 2):
        """Freeze all but last N layers"""
        all_layers = list(model.children())
        
        # Freeze early layers
        for layer in all_layers[:-num_trainable]:
            for param in layer.parameters():
                param.requires_grad = False
        
        # Ensure last layers are trainable
        for layer in all_layers[-num_trainable:]:
            for param in layer.parameters():
                param.requires_grad = True
    
    def _init_ema_params(self):
        """Initialize EMA parameters"""
        self.ema_params = {}
        
        for name, model in [('dynamics', self.dynamics_model),
                           ('outcome', self.outcome_model)]:
            self.ema_params[name] = {}
            for param_name, param in model.named_parameters():
                if param.requires_grad:
                    self.ema_params[name][param_name] = param.data.clone()
    
    def _update_ema(self, model_name: str, model: nn.Module):
        """æŒ‰ç…§è®ºæ–‡å®ç°EMA: Î¸Ì„_{t+1} = Î± Î¸Ì„_t + (1-Î±) Î¸_{t+1}, Î± = 0.99"""
        alpha = 0.99  # è®ºæ–‡ä¸­æ˜ç¡®æåˆ°çš„Î±å€¼
        
        for param_name, param in model.named_parameters():
            if param.requires_grad and param_name in self.ema_params[model_name]:
                # EMAæ›´æ–°å…¬å¼
                self.ema_params[model_name][param_name] = (
                    alpha * self.ema_params[model_name][param_name] +
                    (1 - alpha) * param.data
                )
    
    def process_transition(self, transition: Dict) -> str:
        """
        Process an incoming transition, incorporating active learning and distribution shift detection.
        """
        # 1. Normalize data: Convert any tensors to numpy/python types
        if isinstance(transition['state'], torch.Tensor):
            transition['state'] = transition['state'].cpu().numpy()
        if isinstance(transition['next_state'], torch.Tensor):
            transition['next_state'] = transition['next_state'].cpu().numpy()
        if isinstance(transition['action'], torch.Tensor):
            transition['action'] = transition['action'].cpu().numpy().item()
        if isinstance(transition['reward'], torch.Tensor):
            transition['reward'] = transition['reward'].cpu().numpy().item()
        
        # 2. Update counters and monitor distribution shift (FIX APPLIED)
        self.stats['total_transitions'] += 1
        
        # Add the current state to the distribution shift detector
        self.shift_detector.add_state(transition['state'])
        
        # Every 1000 transitions, check for a significant distribution shift
        if self.stats['total_transitions'] > 0 and self.stats['total_transitions'] % 1000 == 0:
            shift_result = self.shift_detector.detect_shift()
            severity = shift_result.get('severity', 0)
            severity_str = f"{severity:.3f}" if isinstance(severity, (int, float)) else str(severity)
            print(f"Periodic distribution shift check at step {self.stats['total_transitions']}: "
                f"Shift detected: {shift_result.get('shift_detected')}, "
                f"Severity: {severity_str}")
            
            if shift_result.get('shift_detected'):
                # If a shift is detected, trigger the adaptation strategy
                self._handle_distribution_shift(shift_result)
                
        # Print status periodically
        if self.stats['total_transitions'] % 100 == 0:
            print(f"Processed {self.stats['total_transitions']} transitions")

        # 3. Active learning decision logic
        decision, batch = self.active_learner.process_transition(transition)
        
        if decision == 'query':
            self.query_buffer.append(transition)
            self.stats['total_queries'] += 1
            return 'queried'
            
        elif decision == 'query_batch':
            self.query_buffer.extend(batch)
            self.stats['total_queries'] += len(batch)
            return f'queried_batch_{len(batch)}'
            
        elif decision == 'weak':
            self.weak_buffer.append(transition)
            
            # Use a lock to prevent race conditions when triggering training
            with self.training_lock:
                current_time = time.time()
                # Check if there's enough data and enough time has passed
                if (len(self.labeled_buffer) >= self.batch_size and
                    current_time - self.last_train_time > 1.0):
                    
                    self.training_queue.put('train')
                    self.last_train_time = current_time
                    return 'training'
            
            return 'buffered'
        
        return 'buffered'
    def _handle_distribution_shift(self, shift_result: Dict):
        """å¤„ç†æ£€æµ‹åˆ°çš„åˆ†å¸ƒåç§»"""
        print("ğŸ”„ Adapting to distribution shift...")
        
        # 1. é™ä½ä¸ç¡®å®šæ€§é˜ˆå€¼ï¼Œå¢åŠ æŸ¥è¯¢é¢‘ç‡
        current_tau = self.active_learner.get_statistics().get('current_threshold', 0.1)
        new_tau = max(current_tau * 0.7, 0.02)  # é™ä½30%ä½†ä¸ä½äº0.02
        self.active_learner.update_threshold(new_tau)
        
        # 2. å¢åŠ å­¦ä¹ ç‡ä»¥å¿«é€Ÿé€‚åº”
        self.update_hyperparameters({'learning_rate': 5e-4})  # ä¸´æ—¶æé«˜å­¦ä¹ ç‡
        
        # 3. æ¸…ç†æ—§çš„ç»éªŒé‡æ”¾ç¼“å†²åŒº
        if hasattr(self, 'q_trainers'):
            for q_trainer in self.q_trainers:
                # ä¿ç•™æœ€è¿‘çš„ç»éªŒï¼Œæ¸…ç†æ—§ç»éªŒ
                if len(q_trainer.replay_buffer) > 5000:
                    recent_experiences = list(q_trainer.replay_buffer)[-5000:]
                    q_trainer.replay_buffer.clear()
                    q_trainer.replay_buffer.extend(recent_experiences)
        
        print(f"âœ… Adaptation complete: tau={new_tau:.3f}, lr=5e-4")  

    def add_labeled_transition(self, transition: Dict, source: str = 'expert'):
        """Add expert-labeled transition"""
        transition['label_source'] = source
        transition['label_time'] = time.time()
        self.labeled_buffer.append(transition)
    
    def _training_loop(self):
        """Background training loop with mixed blocking/non-blocking approach."""
        while self.is_running:
            try:
                # éé˜»å¡æ£€æŸ¥é˜Ÿåˆ—å‘½ä»¤
                try:
                    command = self.training_queue.get_nowait()
                    if command is None or command == 'stop':
                        break
                    elif command == 'save':
                        self._save_checkpoint()
                    elif command == 'train':
                        pass  # ç»§ç»­æ‰§è¡Œä¸‹é¢çš„è®­ç»ƒé€»è¾‘
                except queue.Empty:
                    pass  # æ²¡æœ‰å‘½ä»¤ï¼Œç»§ç»­æ­£å¸¸è®­ç»ƒ
                
                # ä¸»åŠ¨æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿæ•°æ®è¿›è¡Œè®­ç»ƒ
                if len(self.labeled_buffer) >= min(self.batch_size, 8):
                    self._train_step()
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦ä¿å­˜
                if time.time() - self.stats['last_save_time'] > self.save_freq:
                    self._save_checkpoint()
                    self.stats['last_save_time'] = time.time()
                    
                # é€‚å½“çš„ç¡çœ æ—¶é—´
                time.sleep(0.1)
                    
            except Exception as e:
                print(f"Training loop error: {e}")
                time.sleep(1.0)
    
    def _train_step(self):
        """Executes a training step, incorporating progressive learning adjustments."""
        start_time = time.time()

        # ----------------- Batch Preparation -----------------
        # FIX: The check should be against the minimum required batch size, not the full batch size.
        # The trigger condition is min(self.batch_size, 8), so we match that here.
        if len(self.labeled_buffer) < min(self.batch_size, 8):
            return
        actual_batch_size = min(self.batch_size, len(self.labeled_buffer))
        print(f"Using actual batch size: {actual_batch_size}")  

        # Sample batch
        indices = np.random.choice(len(self.labeled_buffer), actual_batch_size, replace=True)
        batch = [self.labeled_buffer[i] for i in indices]

        # Convert to tensors
        states = torch.stack([torch.FloatTensor(t['state']) for t in batch]).to(self.device)
        actions = torch.LongTensor([t['action'] for t in batch]).to(self.device)
        rewards = torch.FloatTensor([t['reward'] for t in batch]).to(self.device)
        next_states = torch.stack([torch.FloatTensor(t['next_state']) for t in batch]).to(self.device)

        # ----------------- Model Training -----------------
        # Update dynamics model
        if hasattr(self.dynamics_trainer, 'train_online'):
            sequences = self._create_sequences(batch)
            if sequences:
                dynamics_loss = self.dynamics_trainer.train_online(sequences)
                self._update_ema('dynamics', self.dynamics_model)

        # Update outcome model
        outcome_batch = {
            'state': states,
            'action': actions,
            'reward': rewards
        }
        outcome_loss = self.outcome_trainer.train_step_online(outcome_batch)
        self._update_ema('outcome', self.outcome_model)

        # Update Q-networks
        # RLæ¨¡å‹æ›´æ–° (BCQ vs CQL)   
        rl_metrics = {}
        
        if self.use_bcq and self.bcq_trainer:
            # BCQåœ¨çº¿è®­ç»ƒ
            for t in batch:
                bcq_result = self.bcq_trainer.add_transition(t)
                
                # --- ä½¿ç”¨æ–°çš„ã€ç»Ÿä¸€çš„æ‰“å°é€»è¾‘æ›¿æ¢æ—§ç‰ˆ ---
                if bcq_result.get('updated'):
                    algo_name = getattr(self.bcq_trainer, "_algo_name", "bcq")
                    print(
                        f"[{algo_name.upper()}] updated: avg_loss={bcq_result.get('avg_loss', 0):.4f}, "
                        f"updates={bcq_result.get('update_count', -1)}, "
                        f"buffer={bcq_result.get('buffer_size', -1)}"
                    )
                else:
                    # ä¿ç•™å¯¹â€œè·³è¿‡æ›´æ–°â€åŸå› çš„æ‰“å°
                    reason = bcq_result.get('reason') or bcq_result.get('error')
                    if reason:
                        # æ‚¨å¯ä»¥è€ƒè™‘å°†è¿™é‡Œçš„ "BCQ" ä¹Ÿæ›¿æ¢ä¸ºåŠ¨æ€åç§°
                        algo_name = getattr(self.bcq_trainer, "_algo_name", "bcq")
                        print(f"[{algo_name.upper()}] update skipped: {reason}")
            
            rl_metrics = self.bcq_trainer.get_statistics()
        else:
            # CQLè®­ç»ƒ (åŸæœ‰é€»è¾‘)
            for q_trainer in self.q_trainers:
                for t in batch:
                    transition = {
                        'state': torch.FloatTensor(t['state']),
                        'action': torch.tensor(t['action'], dtype=torch.long),
                        'reward': torch.tensor(t['reward'], dtype=torch.float32),
                        'next_state': torch.FloatTensor(t['next_state'])
                    }
                    q_trainer.add_to_replay_buffer(transition)
                
                # Multiple gradient steps for data efficiency
                for _ in range(min(20, max(1, len(q_trainer.replay_buffer) // 100))):
                    q_trainer.train_step()
        
        # ----------------- EMA Application -----------------
        # Every 100 steps, apply the stable EMA weights back to the online models
        if self.stats['total_updates'] > 0 and self.stats['total_updates'] % 100 == 0:
            self._apply_ema_weights()
            print(f"Applied EMA weights at step {self.stats['total_updates']}.")

        # ----------------- Statistics and Adaptive Logic -----------------
        # (FIX APPLIED) Check for and apply scheduled learning phase changes
        update_info = self.learning_scheduler.should_update_hyperparams(
            self.stats['total_transitions']
        )
        if update_info:
            print(f"\nğŸ“š Entering '{update_info['phase']}' learning phase at transition {self.stats['total_transitions']}")
            
            # Adjust learning rate based on the schedule
            new_lr = self.base_learning_rate * update_info['learning_rate_multiplier']
            self.update_hyperparameters({'learning_rate': new_lr})
            
            # Adjust uncertainty threshold (tau) based on the schedule
            new_tau = self.base_tau * update_info['tau_multiplier']
            self.active_learner.update_threshold(new_tau)
            
            print(f"  - Learning rate set to: {new_lr:.2e}")
            print(f"  - Uncertainty threshold (tau) set to: {new_tau:.3f}\n")

        # Update step statistics
        self.stats['total_updates'] += 1
        self.stats['training_times'].append(time.time() - start_time)

        # Log evaluation history periodically
        if self.stats['total_updates'] % 10 == 0:
            self.evaluation_history.append({
                'timestamp': time.time(),
                'active_learning': self.active_learner.get_statistics(),
                'performance': np.mean(list(self.stats['training_times'])) if self.stats['training_times'] else 0
            })

        # Adaptively fine-tune the active learning threshold based on recent query rate
        if self.stats['total_updates'] % 50 == 0:
            al_stats = self.active_learner.get_statistics()
            current_query_rate = al_stats.get('query_rate', 0)
            current_tau = al_stats.get('current_threshold', 0.05)
            
            print(f"[ADAPTIVE] Update {self.stats['total_updates']}: Query rate: {current_query_rate:.1%}, Tau: {current_tau:.3f}")
            
            if current_query_rate > 0.2:
                new_tau = min(current_tau * 1.2, 0.8)
                self.active_learner.update_threshold(new_tau)
                print(f"[ADAPTIVE] Query rate high, increasing tau to {new_tau:.3f}")
            
            # If query rate is too low, decrease threshold to query more
            elif current_query_rate < 0.05:
                new_tau = max(current_tau * 0.8, 0.01)  # Decrease by 20%, with a floor of 0.01
                self.active_learner.update_threshold(new_tau)
                print(f"Query rate low ({current_query_rate:.1%}). Fine-tuning tau to {new_tau:.3f}")


    def _apply_ema_weights(self):
        """å°†EMAæƒé‡åº”ç”¨åˆ°æ¨¡å‹ï¼ˆç”¨äºç¨³å®šæ€§ï¼‰"""
        for model_name, model in [('dynamics', self.dynamics_model), 
                                ('outcome', self.outcome_model)]:
            if model_name in self.ema_params:
                for param_name, param in model.named_parameters():
                    if param.requires_grad and param_name in self.ema_params[model_name]:
                        # ä¸´æ—¶ä¿å­˜å½“å‰æƒé‡å¹¶åº”ç”¨EMAæƒé‡
                        param.data.copy_(self.ema_params[model_name][param_name])
                        print(f"Applied EMA weight for {model_name}.{param_name}")
                        
    def _create_sequences(self, batch: List[Dict]) -> List[Dict]:
        """Create sequences from transitions for dynamics training"""
        # Group by trajectory if available
        sequences = []
        
        # For now, treat each transition independently
        # In production, you'd group by patient trajectory
        for trans in batch[:10]:  # Limit sequences
            seq = {
                'states': torch.stack([
                    torch.FloatTensor(trans['state']),
                    torch.FloatTensor(trans['next_state'])
                ]),
                'actions': torch.tensor([trans['action']]),
                'length': 2
            }
            sequences.append(seq)
        
        return sequences
    
    def _save_checkpoint(self):
        """Save model checkpoint"""
        checkpoint_dir = './checkpoints/online'
        os.makedirs(checkpoint_dir, exist_ok=True)
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # Save models
        torch.save(self.dynamics_model.state_dict(), 
                  f'{checkpoint_dir}/dynamics_{timestamp}.pth')
        torch.save(self.outcome_model.state_dict(), 
                  f'{checkpoint_dir}/outcome_{timestamp}.pth')
        torch.save(self.q_ensemble.state_dict(), 
                  f'{checkpoint_dir}/q_ensemble_{timestamp}.pth')
        
        # Save EMA parameters
        torch.save(self.ema_params, 
                  f'{checkpoint_dir}/ema_{timestamp}.pth')
        
        # Save statistics
        stats_to_save = self.get_statistics()
        # FIX: Convert deque to list for JSON serialization
        if 'training_times' in stats_to_save and isinstance(stats_to_save['training_times'], deque):
            stats_to_save['training_times'] = list(stats_to_save['training_times'])

        with open(f'{checkpoint_dir}/stats_{timestamp}.json', 'w') as f:
            json.dump(stats_to_save, f, indent=2)
        
        print(f"Checkpoint saved at {timestamp}")
    
    def get_statistics(self) -> Dict:
        """Get training statistics"""
        stats = self.stats.copy()
        
        # Add buffer sizes
        stats['labeled_buffer_size'] = len(self.labeled_buffer)
        stats['weak_buffer_size'] = len(self.weak_buffer)
        stats['query_buffer_size'] = len(self.query_buffer)
        
        # Add timing stats
        if self.stats['training_times']:
            stats['avg_training_time'] = np.mean(list(self.stats['training_times']))
            stats['max_training_time'] = np.max(list(self.stats['training_times']))
        
        # Add active learning stats
        stats.update(self.active_learner.get_statistics())
        
        return stats
    
    def update_hyperparameters(self, params: Dict):
        """Update training hyperparameters on the fly"""
        if 'learning_rate' in params:
            # Update all optimizers
            for trainer in [self.dynamics_trainer] + self.q_trainers:
                for param_group in trainer.optimizer.param_groups:
                    param_group['lr'] = params['learning_rate']
            
            if hasattr(self.outcome_trainer, 'optimizer'):
                for param_group in self.outcome_trainer.optimizer.param_groups:
                    param_group['lr'] = params['learning_rate']
        
        if 'batch_size' in params:
            self.batch_size = params['batch_size']
        
        if 'tau' in params:
            self.active_learner.update_threshold(params['tau'])
        
        if 'cql_weight' in params:
            for trainer in self.q_trainers:
                trainer.cql_weight = params['cql_weight']
        
        print(f"Updated hyperparameters: {params}")
    
    def stop(self):
        """Stops the training loop by sending a sentinel value to the queue."""
        print("Attempting to stop the online training thread...")
        if self.is_running:
            self.is_running = False
            # Put a sentinel value on the queue to unblock the thread's get() call.
            self.training_queue.put(None) 
            if self.training_thread:
                # Wait for the thread to finish its work.
                self.training_thread.join(timeout=5) 
                if self.training_thread.is_alive():
                    print("Warning: Training thread did not stop gracefully.")
        print("Online training stopped")



class ProgressiveLearningScheduler:
    """æ¸è¿›å¼å­¦ä¹ è°ƒåº¦å™¨"""
    
    def __init__(self):
        self.learning_phases = {
            'bootstrap': {'duration': 1000, 'lr_multiplier': 2.0, 'tau_multiplier': 0.5},
            'exploration': {'duration': 3000, 'lr_multiplier': 1.5, 'tau_multiplier': 0.8},
            'exploitation': {'duration': float('inf'), 'lr_multiplier': 1.0, 'tau_multiplier': 1.0}
        }
        
        self.current_phase = 'bootstrap'
        self.phase_start_time = 0
        self.transition_count = 0
        
    def update_phase(self, total_transitions: int) -> bool:
        """æ›´æ–°å­¦ä¹ é˜¶æ®µ"""
        phase_changed = False
        
        if self.current_phase == 'bootstrap' and total_transitions >= 1000:
            self.current_phase = 'exploration'
            self.transition_count = total_transitions
            phase_changed = True
            print("ğŸ”„ Transitioning to EXPLORATION phase")
            
        elif self.current_phase == 'exploration' and total_transitions >= 4000:
            self.current_phase = 'exploitation'
            self.transition_count = total_transitions
            phase_changed = True
            print("ğŸ”„ Transitioning to EXPLOITATION phase")
        
        return phase_changed
    
    def get_current_multipliers(self) -> Dict:
        """è·å–å½“å‰é˜¶æ®µçš„å‚æ•°ä¹˜æ•°"""
        return self.learning_phases[self.current_phase]
    
    def should_update_hyperparams(self, total_transitions: int) -> Optional[Dict]:
        """æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°è¶…å‚æ•°"""
        if self.update_phase(total_transitions):
            multipliers = self.get_current_multipliers()
            return {
                'learning_rate_multiplier': multipliers['lr_multiplier'],
                'tau_multiplier': multipliers['tau_multiplier'],
                'phase': self.current_phase
            }
        return None

class DistributionShiftDetector:
    """æ£€æµ‹æ‚£è€…ç¾¤ä½“åˆ†å¸ƒåç§»"""
    
    def __init__(self, window_size=1000, shift_threshold=0.1):
        self.window_size = window_size
        self.shift_threshold = shift_threshold
        self.reference_states = deque(maxlen=window_size)
        self.current_states = deque(maxlen=window_size//2)
        self.last_check_time = time.time()
        
    def add_state(self, state: np.ndarray):
        """æ·»åŠ æ–°çŠ¶æ€åˆ°ç›‘æ§"""
        if len(self.reference_states) < self.window_size:
            self.reference_states.append(state)
        else:
            self.current_states.append(state)
            
    def detect_shift(self) -> Dict:
            """ä½¿ç”¨Kolmogorov-Smirnovæµ‹è¯•æ£€æµ‹åˆ†å¸ƒåç§»"""
            if len(self.current_states) < 100:  # éœ€è¦è¶³å¤Ÿæ ·æœ¬
                return {'shift_detected': False, 'p_value': 1.0, 'severity': 0.0, 'affected_dimensions': []}
                
            from scipy import stats
            
            ref_data = np.array(list(self.reference_states))
            curr_data = np.array(list(self.current_states))
            
            # å¯¹æ¯ä¸ªç‰¹å¾ç»´åº¦è¿›è¡ŒKSæµ‹è¯•
            p_values = []
            for dim in range(ref_data.shape[1]):
                _, p_val = stats.ks_2samp(ref_data[:, dim], curr_data[:, dim])
                p_values.append(p_val)
                
            min_p_value = min(p_values)
            shift_detected = min_p_value < self.shift_threshold
            
            if shift_detected:
                print(f"ğŸš¨ Distribution shift detected! p-value: {min_p_value:.4f}")
                
            # è®¡ç®—ä¸¥é‡ç¨‹åº¦
            severity = 1.0 - min_p_value if shift_detected else 0.0
            return {
                'shift_detected': shift_detected,
                'p_value': min_p_value,
                'severity': severity,
                'affected_dimensions': [i for i, p in enumerate(p_values) if p < self.shift_threshold]
            }

class OnlineDataStream:
    """Simulates streaming patient data"""
    
    def __init__(self, 
                 data_source: Callable,
                 stream_rate: float = 1.0,
                 add_noise: bool = True):
        """
        Initialize data stream
        
        Args:
            data_source: Function that returns transitions
            stream_rate: Transitions per second
            add_noise: Add realistic noise to transitions
        """
        self.data_source = data_source
        self.stream_rate = stream_rate
        self.add_noise = add_noise
        self.is_streaming = False
        self.stream_thread = None
        self.callbacks = []
        self.training_lock = threading.Lock()
        self.last_train_time = 0

    def add_callback(self, callback: Callable):
        """Add callback for new transitions"""
        self.callbacks.append(callback)
    
    def start_stream(self):
        """Start data streaming"""
        self.is_streaming = True
        self.stream_thread = threading.Thread(target=self._stream_loop, daemon=True)
        self.stream_thread.start()
        print(f"Data stream started at {self.stream_rate} transitions/sec")
    
    def stop_stream(self):
        """Stop data streaming"""
        self.is_streaming = False
        if self.stream_thread:
            self.stream_thread.join()
        print("Data stream stopped")
    
    def _stream_loop(self):
        """Main streaming loop with proper rate control"""
        while self.is_streaming:
            loop_start = time.perf_counter()
            
            # è·å–å’Œå¤„ç†æ•°æ®
            transition = self.data_source()
            if transition is None:
                time.sleep(0.1)
                continue
            
            if self.add_noise:
                transition = self._add_noise(transition)
            
            # è°ƒç”¨å›è°ƒå‡½æ•°
            for callback in self.callbacks:
                try:
                    callback(transition)
                except Exception as e:
                    print(f"Callback error: {e}")
            
            # ç²¾ç¡®çš„é€Ÿç‡æ§åˆ¶
            process_time = time.perf_counter() - loop_start
            target_interval = 1.0 / self.stream_rate
            sleep_time = max(0, target_interval - process_time)
            
            if sleep_time > 0:
                time.sleep(sleep_time)
    
    def _add_noise(self, transition: Dict) -> Dict:
        """Add realistic noise to transition"""
        noisy_transition = transition.copy()
        
        # Add small Gaussian noise to states
        if 'state' in transition:
            noise = np.random.normal(0, 0.01, size=transition['state'].shape)
            noisy_transition['state'] = transition['state'] + noise
            noisy_transition['state'] = np.clip(noisy_transition['state'], 0, 1)
        
        if 'next_state' in transition:
            noise = np.random.normal(0, 0.01, size=transition['next_state'].shape)
            noisy_transition['next_state'] = transition['next_state'] + noise
            noisy_transition['next_state'] = np.clip(noisy_transition['next_state'], 0, 1)
        
        return noisy_transition


class ExpertSimulator:
    """Simulates expert labeling with delay"""
    
    def __init__(self,
                 label_delay: float = 2.0,
                 accuracy: float = 0.95):
        """
        Initialize expert simulator
        
        Args:
            label_delay: Seconds to wait before returning label
            accuracy: Probability of correct label
        """
        self.label_delay = label_delay
        self.accuracy = accuracy
        self.label_queue = queue.Queue()
        self.is_running = True
        self.label_thread = threading.Thread(target=self._label_loop, daemon=True)
        self.label_thread.start()
    
    def request_label(self, transition: Dict, callback: Callable):
        """Request expert label for transition"""
        self.label_queue.put({
            'transition': transition,
            'callback': callback,
            'request_time': time.time()
        })
    
    def _label_loop(self):
        """Process labeling requests"""
        while self.is_running:
            try:
                request = self.label_queue.get(timeout=1.0)
                
                # æ·»åŠ è°ƒè¯•
                print(f"Expert labeling request received")
                
                # Simulate delay
                time.sleep(self.label_delay)
                
                # Generate label (possibly with error)
                labeled_transition = self._generate_label(request['transition'])
                
                # Call callback
                request['callback'](labeled_transition)
                
                # æ·»åŠ è°ƒè¯•
                print(f"Expert label provided")
                
            except queue.Empty:
                continue
    
    def _generate_label(self, transition: Dict) -> Dict:
        """Generate expert label"""
        labeled = transition.copy()
        
        # Simulate expert providing true reward
        if np.random.rand() < self.accuracy:
            # Correct label - compute true reward
            labeled['reward'] = self._compute_true_reward(transition)
            labeled['label_quality'] = 'expert_verified'
        else:
            # Noisy label
            labeled['reward'] = transition.get('reward', 0) + np.random.normal(0, 0.1)
            labeled['label_quality'] = 'expert_noisy'
        
        return labeled
    
    def _compute_true_reward(self, transition: Dict) -> float:
        """Compute true reward for transition"""
        # Simplified reward based on state improvement
        state = transition['state']
        next_state = transition['next_state']
        
        # Health improvement
        health_improvement = 0.0
        
        # Oxygen saturation (most critical)
        o2_improvement = (next_state[8] - state[8]) * 5.0
        health_improvement += o2_improvement
        
        # Other vitals
        for i in [2, 3, 4, 5]:  # BP, HR, glucose, creatinine
            # Reward moving toward normal (0.5)
            old_deviation = abs(state[i] - 0.5)
            new_deviation = abs(next_state[i] - 0.5)
            improvement = old_deviation - new_deviation
            health_improvement += improvement * 2.0
        
        return health_improvement
    
    def stop(self):
        """Stop expert simulator"""
        self.is_running = False
        self.label_thread.join()

from data import PatientDataGenerator
def create_online_training_system(models: Dict,
                                  sampler_type: str = 'hybrid',
                                  tau: float = 0.05,
                                  stream_rate: float = 1.0) -> Dict:
    """
    Create complete online training system with distribution shift simulation.
    
    Args:
        models: Dict with dynamics_model, outcome_model, q_ensemble
        sampler_type: Type of active learning sampler
        tau: Uncertainty threshold
        stream_rate: Data streaming rate
        
    Returns:
        Dict with trainer, stream, expert, and controller
    """
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    
    # Create active learner
    # æ£€æŸ¥BCQæ˜¯å¦å¯ç”¨ï¼Œè°ƒæ•´active learneré…ç½®
    bcq_path = os.path.join('./output/models', 'best_bcq_policy.d3')
    use_bcq = os.path.exists(bcq_path) and BCQ_AVAILABLE
    
    if use_bcq:
        print("âœ“ Found BCQ policy, adapting active learner")
        # BCQæƒ…å†µä¸‹ï¼Œé€‚å½“è°ƒæ•´taué˜ˆå€¼
        tau = max(tau, 0.1)  # BCQéœ€è¦æ›´é«˜çš„é˜ˆå€¼
    
    # Create active learner
    active_learner = StreamActiveLearner(
        models['q_ensemble'],
        sampler_type=sampler_type,
        tau=tau,
        device=device
    )
    
    # Create online trainer
    trainer = OnlineTrainer(
        models['dynamics_model'],
        models['outcome_model'],
        models['q_ensemble'],
        active_learner,
        device=device
    )
    
    # Create expert simulator
    expert = ExpertSimulator(label_delay=0.1)

    # --- START: MODIFIED DATA GENERATION LOGIC ---

    # åˆ›å»ºä¸¤ä¸ªä¸åŒçš„æ•°æ®ç”Ÿæˆå™¨
    offline_generator = PatientDataGenerator(n_patients=1000, seed=42)
    online_generator = PatientDataGenerator(n_patients=5000, seed=123) # ä¸åŒçš„seedå’Œæ‚£è€…
    
    # é¢„å…ˆç”Ÿæˆç¦»çº¿æ•°æ®é›†
    full_dataset = offline_generator.generate_dataset()

    # æ¨¡æ‹Ÿåˆ†å¸ƒåç§»
    distribution_shift_time = 5000
    transition_count = [0]

    def data_generator():
        # è·Ÿè¸ªæ€»å…±ç”Ÿæˆäº†å¤šå°‘è½¬æ¢
        current_count = transition_count[0]
        
        # å‰5000ä¸ªæ ·æœ¬ä½¿ç”¨åŸå§‹åˆ†å¸ƒ
        if current_count < distribution_shift_time:
            idx = current_count % len(full_dataset['states'])
            transition = {
                'state': full_dataset['states'][idx],
                'action': full_dataset['actions'][idx],
                'reward': full_dataset['rewards'][idx],
                'next_state': full_dataset['next_states'][idx]
            }
        else:
            # 5000ä¸ªæ ·æœ¬åï¼Œæ¨¡æ‹Ÿåˆ†å¸ƒåç§»ï¼ˆä¾‹å¦‚æ›´å¤šè€å¹´æ‚£è€…ï¼‰
            state = online_generator._generate_initial_state()
            state[0] = np.clip(state[0] + 0.3, 0, 1)  # å¢åŠ å¹´é¾„
            action = online_generator._behavior_policy(state)
            next_state = online_generator._transition_dynamics(state, action)
            reward = online_generator._compute_reward(state, action, next_state)
            
            transition = {
                'state': state,
                'action': action,
                'reward': reward,
                'next_state': next_state
            }
        
        # æ¯æ¬¡è°ƒç”¨éƒ½å¢åŠ è®¡æ•°å™¨
        transition_count[0] += 1
        return transition

    # --- END: MODIFIED DATA GENERATION LOGIC ---
    
    stream = OnlineDataStream(data_generator, stream_rate=stream_rate)
    
    # Connect components
    def process_transition(transition):
        status = trainer.process_transition(transition)
        
        if 'queried' in status:
            # ä¿®æ”¹å›è°ƒä»¥è§¦å‘è®­ç»ƒæ£€æŸ¥
            def labeled_callback(labeled_trans):
                trainer.add_labeled_transition(labeled_trans)
                # æ·»åŠ ï¼šæ£€æŸ¥æ˜¯å¦å¯ä»¥è®­ç»ƒ
                if len(trainer.labeled_buffer) >= min(trainer.batch_size, 8):
                    trainer.training_queue.put('train')
                    print(f"Labeled buffer size: {len(trainer.labeled_buffer)}, triggering training")
            
            expert.request_label(transition, labeled_callback)
    
    stream.add_callback(process_transition)
    
    return {
        'trainer': trainer,
        'stream': stream,
        'expert': expert,
        'active_learner': active_learner
    }


# Extension for DigitalTwinTrainer to support online updates
def train_online(self, sequences: List[Dict]) -> float:
    """Online training step for dynamics model"""
    self.model.train()
    total_loss = 0.0
    
    for seq in sequences:
        states = seq['states'].unsqueeze(0).to(self.device)
        if states.shape[1] < 2:
            continue
            
        # Single action for transition
        actions = seq['actions'].unsqueeze(0).to(self.device)
        
        # Forward pass - predict next state
        predicted_next = self.model(states[:, :-1], actions)
        target_next = states[:, 1:]
        
        # Loss
        loss = nn.functional.mse_loss(predicted_next, target_next)
        
        # Backward
        self.optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
        self.optimizer.step()
        
        total_loss += loss.item()
    
    return total_loss / len(sequences) if sequences else 0.0


# Monkey patch the method
DigitalTwinTrainer.train_online = train_online


# Extension for OutcomeModelTrainer to support online updates
def train_step_online(self, batch: Dict) -> float:
    """Single online training step"""
    self.model.train()
    
    states = batch['state']
    actions = batch['action']
    rewards = batch['reward']
    
    # Forward
    predicted_rewards = self.model(states, actions).squeeze()
    
    # Loss
    loss = nn.functional.mse_loss(predicted_rewards, rewards)
    
    # Add regularization if needed
    reg_loss = self.model.compute_regularization_loss(states, actions)
    total_loss = loss + self.regularization_weight * reg_loss
    
    # Backward
    self.optimizer.zero_grad()
    total_loss.backward()
    torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
    self.optimizer.step()
    
    return total_loss.item()


# Monkey patch the method
OutcomeModelTrainer.train_step_online = train_step_online


if __name__ == "__main__":
    print("Testing Online Training System...")
    
    # Create mock models
    from models import TransformerDynamicsModel, TreatmentOutcomeModel, EnsembleQNetwork
    
    state_dim = 10
    action_dim = 5
    
    models = {
        'dynamics_model': TransformerDynamicsModel(state_dim, action_dim),
        'outcome_model': TreatmentOutcomeModel(state_dim, action_dim),
        'q_ensemble': EnsembleQNetwork(state_dim, action_dim)
    }
    
    # Create online system
    system = create_online_training_system(models, stream_rate=2.0)
    
    # Start streaming
    system['stream'].start_stream()
    
    print("System running. Press Ctrl+C to stop...")
    
    try:
        # Run for a bit
        time.sleep(10)
        
        # Print statistics
        stats = system['trainer'].get_statistics()
        print("\nTraining Statistics:")
        for k, v in stats.items():
            print(f"  {k}: {v}")
        
    except KeyboardInterrupt:
        print("\nStopping...")
    
    # Cleanup
    system['stream'].stop_stream()
    system['trainer'].stop()
    system['expert'].stop()
    
    print("Test complete!")